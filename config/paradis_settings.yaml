model:
  hidden_multiplier: 4        # Automatically scale the hidden dimension with the number of input channels (hidden_dim = hidden_multiplier * nb_dynamic_channels)
  forecast_steps: 1           # Number of autoregressive steps for forecasting
  base_dt: 21600              # 6 hours * 3600 seconds
  loss_alpha: 0.5             # Balance the contribution MAE and RMSE. loss = α * MAE + (1-α) * RMSE
  checkpoint_path: null       # Path to model checkpoint for loading pre-trained weights
  compile: true               # Whether to compile the model using torch.compile()
  variational: False          # Flag for variational variant
  beta: 0.01                  # Beta from beta-vae

trainer:
  lr: 5.0e-4                  # Learning rate (LR) for the optimizer
  weight_decay: 1.e-5         # Weight decay coefficient for regularization
  gradient_clip_val: -1       # Gradient clipping value, set to -1 to disable
  max_epochs: 100
  num_devices: 1
  accelerator: gpu
  use_amp: true               # Use Automatic Mixed Precision
  print_losses: false
  early_stopping:
    enabled: true             # Whether to use early stopping
    patience: 8               # Number of epochs with no improvement after which training will stop

  # Scheduler configuration - Choose ONE of the following scheduler types by
  # commenting/uncommenting the appropriate section:

  # 1. OneCycleLR - Recommended for initial training
  scheduler:
    type: "one_cycle"         # OneCycleLR scheduler
    warmup_pct_start: 0.      # Percentage of training used for warmup phase
    lr_div_factor: 25         # Initial learning rate = max_lr/lr_div_factor
    lr_final_div: 1.0e2       # Final learning rate = max_lr/lr_final_div_factor

  # 2. ReduceLROnPlateau - Recommended for fine-tuning a pre-trained model
  # scheduler:
  #   type: "reduce_lr"       # ReduceLROnPlateau scheduler
  #   factor: 0.5             # Factor by which to reduce learning rate
  #   patience: 5             # Number of epochs with no improvement after which LR will be reduced
  #   min_lr: 1e-6            # Lower bound on the learning rate

dataset:
  dataset_dir: /home/cap003/hall6/weatherbench_paradis/era5_5.625deg_13level/
  start_date: 2000-01-01  # Start date for the dataset (YYYY-MM-DD)
  end_date: 2000-12-31    # End date for the dataset (YYYY-MM-DD)
  train_ratio: 0.8        # Ratio of data used for training (rest for validation)
  batch_size: 64          # Number of samples per batch
  num_workers: 16          # Number of workers for dataloader

features:
  pressure_levels: # Added to atmospheric variable names
    - 50
    - 100
    - 150
    - 200
    - 250
    - 300
    - 400
    - 500
    - 600
    - 700
    - 850
    - 925
    - 1000

  base:
    atmospheric:
      - geopotential
      - wind_x
      - wind_y
      - wind_z
      - specific_humidity
      - temperature
    surface:
      - wind_x_10m
      - wind_y_10m
      - 2m_temperature
      - mean_sea_level_pressure
      - surface_pressure
      - total_column_water
      - total_precipitation_6hr

  input:
    atmospheric: ${features.base.atmospheric}
    surface: ${features.base.surface}
    forcings:
      - toa_incident_solar_radiation
      - sin_time_of_day
      - cos_time_of_day
      - sin_year_progress
      - cos_year_progress
    constants:
      - geopotential_at_surface
      - land_sea_mask
      - slope_of_sub_gridscale_orography
      - standard_deviation_of_orography
      - latitude
      - longitude

  output:
    atmospheric: ${features.base.atmospheric}
    surface: ${features.base.surface}

variable_loss_weights:
  atmospheric:
    wind_x: 1.0
    wind_y: 1.0
    wind_z: 1.0
    geopotential: 1.0
    specific_humidity: 1.0
    temperature: 1.0
  surface:
    wind_x_10m: 0.1
    wind_y_10m: 0.1
    2m_temperature: 0.1
    mean_sea_level_pressure: 0.1
    surface_pressure: 0.1
    total_column_water: 0.1
    total_precipitation_6hr: 0.1
