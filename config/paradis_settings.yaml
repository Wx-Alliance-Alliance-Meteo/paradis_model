# Model architecture
model:
  latent_multiplier: -1            # Scale the dynamic inputs in hidden dimension with the number of input channels
  latent_size: 1024                # Set a fixed size of the latent space (if latent_multiplier is -1)
  forecast_steps: 1                # Number of autoregressive steps
  base_dt: 21600                   # Data time resolution in seconds (6 hours * 3600 seconds)
  num_layers: 8                    # Model depth
  bias_channels: 4                 # Number of bias channels
  velocity_vectors: 256            # Number of velocity layers (if latent_multiplier is -1)
  diffusion_size: 1024             # Latent size for the diffusion operator
  reaction_size: 1024              # Size for the reaction operator
  adv_interpolation: bicubic       # Choose between bilinear or bicubic
  projected_advection: true        # Whether to project the advection layers to velocity_vectors space
  num_encoder_layers: 1            # Number of input encoding layers

# Initialization settings (seeding)
init:
  seed: 42                         # If not provided, a seed will not be used
  checkpoint_path: null            # Path to model checkpoint for loading pre-trained weights
  restart: false                   # Whether to restart from checkpoint (true) or only load pre-trained weights (false)

# Input dataset
dataset:
  root_dir: /path/to/dataset/
  time_resolution: 6h              # Dataset original resolution
  n_time_inputs: 2                 # Number inputs (For example, n_time_inputs: 2 feeds [tn-1, tn])
  sampling_interval: 6h            # Sample dataset feeds at this time interval
  prediction_delta: 6h             # Prediction will be done at t+prediction_delta

normalization:
  standard: False                  # (True = z-score normalization to all variables, False = custom norms)

# Compute/hardware aspects
compute:
  num_nodes: 1                     # Number of nodes
  num_devices: 1                   # Number of GPUS/CPU devices per node
  accelerator: gpu                 # CPU/GPU
  use_amp: true                    # Enable AMP (Automatic Mixed Precision)
  batch_size: 32                   # Number of samples per batch
  num_workers: 8                   # Number of parallel processes for data preloading
  compile: true                    # Whether to compile the model

# Forecast parameters
forecast:
  enable: false                    # Set model to forecast mode
  start_date: 2019-12-31           # Time to generate forecast from (YYYY-MM-DD) or (YYYY-MM-DDTHH:MM:SS)
  end_date: null                   # OPTIONAL, for validation purposes (YYYY-MM-DD) or null
  output_frequency: 1              # Store to file every output_frequency forecast steps
  output_file: null                # Name of output forecast file

# Training parameters
training:
  max_epochs: -1                   # Maximum number of epochs the training is allowed (-1 to disable and use steps)
  max_steps: 300000                # Maximum number of steps to use for training (-1 to disable and use epochs)
  progress_bar: true               # Enable progress bar
  gradient_clip_val: null          # Gradient clipping value, set to null to disable
  print_losses: false              # Manually print losses to screen (removes progress bar)
  log_every_n_steps: 100           # Write to log every n steps
  log_dir: logs
  experiment_name: null            # Optional: Custom name for log directory (replaces version_xxx), set to null for default versioning

  dataset:
    start_date: 1990-01-01         # Start date for the training dataset (YYYY-MM-DD)
    end_date: 2019-12-31           # End date for the training dataset (YYYY-MM-DD)
    preload: false                 # If true, load the full dataset from disk into memory to speed training

  validation_dataset:
    start_date: 2020-01-01         # Start date for the validation dataset (YYYY-MM-DD)
    end_date: 2020-12-31           # End date for the validation dataset (YYYY-MM-DD)
    preload: false                 # If true, load the full dataset from disk into memory to speed validation
    validation_every_n_steps: null # Run validation every N steps (null once per epoch)
    validation_batches: null       # If non-null, use N batches rather than all for validation (0 disables val)

  optimizer:
    lr: 2.5e-4                     # Learning rate (LR) for the optimizer
    weight_decay: 1.e-2            # Weight decay coefficient for regularization
    beta1: 0.9                     # Exponential decay rate for the first moment (gradient) estimates
    beta2: 0.95                    # Exponential decay rate for the second moment (squared gradient) estimates

  early_stopping:
    enabled: false                 # Whether to use early stopping
    patience: 8                    # Number of epochs with no improvement after which training will stop

  loss_function:
    type: "reversed_huber"         # mse, reversed_huber or dyn_reversed_huber loss functions
    delta_loss: 1.0                # Threshold parameter for huber loss
    lat_weights: true             # Integrate loss with spherical metrics
    validation_loss: null          # Specify if a different score should be used for validation

  checkpointing:
    enabled: true                  # Enable checkpointing

  # Scheduler configuration - Choose ONE of the following scheduler types by
  # setting enable to true in the appropriate section:

  scheduler:

    # 1. OneCycleLR
    one_cycle:
      enabled: false
      warmup_pct_start: 0.         # Percentage of training used for warmup phase
      lr_div_factor: 1             # Initial learning rate = max_lr/lr_div_factor
      lr_final_div: 100            # Final learning rate = initial_lr/lr_final_div_factor

    # 2. ReduceLROnPlateau
    reduce_lr:
      enabled: false
      factor: 0.75                 # Factor by which to reduce learning rate
      patience: 3                  # Number of epochs with no improvement after which LR will be reduced
      threshold: 1.e-4
      threshold_mode: "rel"
      min_lr: 1.e-7                # Lower bound on the learning rate

    # 3. LambdaLR with initial warmup, steady learning rate and final cooldown
    wsd:
      enabled: true
      warmup: 1000                 # ≥1 : number of warmup steps, else fraction
      decay: 0.2                   # ≥1 : number of decay steps, else fraction

  variable_loss_weights:
    atmospheric:
      wind_x: 1.0
      wind_y: 1.0
      wind_z: 1.0
      geopotential: 1.0
      specific_humidity: 1.0
      temperature: 1.0
      vertical_velocity: 1.0
    surface:
      wind_x_10m: 1.0
      wind_y_10m: 1.0
      wind_z_10m: 1.0
      2m_temperature: 1.0
      mean_sea_level_pressure: 1.0
      surface_pressure: 1.0
      total_column_water: 1.0

  reports:
    enable: true
    features:
      - geopotential_h500

features:
  pressure_levels:
    - 50
    - 100
    - 150
    - 200
    - 250
    - 300
    - 400
    - 500
    - 600
    - 700
    - 850
    - 925
    - 1000

  base:
    atmospheric:
      - geopotential
      - wind_x
      - wind_y
      - wind_z
      - specific_humidity
      - temperature
      - vertical_velocity
    surface:
      - wind_x_10m
      - wind_y_10m
      - wind_z_10m
      - 2m_temperature
      - mean_sea_level_pressure
      - surface_pressure
      - total_column_water

  input:
    atmospheric: ${features.base.atmospheric}
    surface: ${features.base.surface}
    forcings:
      - toa_incident_solar_radiation
      - sin_time_of_day
      - cos_time_of_day
      - sin_year_progress
      - cos_year_progress
    constants:
      - geopotential_at_surface
      - land_sea_mask
      - slope_of_sub_gridscale_orography
      - standard_deviation_of_orography
      - lon_spacing
      - cos_latitude
      - cos_longitude
      - sin_longitude
      - latitude
      - longitude

  output:
    atmospheric: ${features.base.atmospheric}
    surface: ${features.base.surface}

