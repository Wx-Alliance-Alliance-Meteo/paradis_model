model:
  hidden_multiplier: 4    # Automatically scale the hidden dimension with the number of input channels (hidden_dim = hidden_multiplier * nb_dynamic_channels)
  forecast_steps: 8       # Number of autoregressive steps for forecasting
  base_dt: 21600          # 6 hours * 3600 seconds
  loss_alpha: 0.5         # Balance the contribution MAE and RMSE. loss = α * MAE + (1-α) * RMSE
  checkpoint_path: /data/users/jupyter-dam724/paradis_model/logs/lightning_logs/version_8/checkpoints/epoch=99.ckpt   # Path to model checkpoint for loading pre-trained weights
  compile: true           # Whether to compile the model using torch.compile()
  variational: true       # Flag for variational variant
  beta: 0.01              # Beta from beta-vae

trainer:
  lr: 5.0e-4             # Learning rate (LR) for the optimizer
  weight_decay: 0.01     # Weight decay coefficient for regularization
  gradient_clip_val: -1  # Gradient clipping value, set to -1 to disable
  max_epochs: 100
  num_devices: 1
  accelerator: gpu
  use_amp: true          # Use Automatic Mixed Precision
  print_losses: false
  early_stopping:
    enabled: true        # Whether to use early stopping
    patience: 8          # Number of epochs with no improvement after which training will stop

  # Scheduler configuration - Choose ONE of the following scheduler types by
  # commenting/uncommenting the appropriate section:

  # 1. OneCycleLR - Recommended for initial training
  scheduler:
    type: "one_cycle"     # OneCycleLR scheduler
    warmup_pct_start: 0.2 # Percentage of training used for warmup phase
    lr_div_factor: 25     # Initial learning rate = max_lr/lr_div_factor
    lr_final_div: 1.0e3   # Final learning rate = max_lr/lr_final_div_factor

  # 2. ReduceLROnPlateau - Recommended for fine-tuning a pre-trained model
#  scheduler:
#    type: "reduce_lr"      # ReduceLROnPlateau scheduler
#    factor: 0.5            # Factor by which to reduce learning rate
#    patience: 5            # Number of epochs with no improvement after which LR will be reduced
#    min_lr: 1e-6           # Lower bound on the learning rate

dataset:
  dataset_dir: ERA5/5.65deg
  start_date: 1960-01-01  # Start date for the dataset (YYYY-MM-DD)
  end_date: 2020-12-31    # End date for the dataset (YYYY-MM-DD)
  train_ratio: 0.8        # Ratio of data used for training (rest for validation)
  batch_size: 64          # Number of samples per batch
  num_workers: 4          # Number of workers for dataloader

features:
  pressure_levels: # Added to atmospheric variable names
    - 50
    - 100
    - 150
    - 200
    - 250
    - 300
    - 400
    - 500
    - 600
    - 700
    - 850
    - 925
    - 1000

  base:
    atmospheric:
      - geopotential
      - u_component_of_wind
      - v_component_of_wind
      - specific_humidity
      - temperature
      - vertical_velocity

  input:
    atmospheric: ${features.base.atmospheric} # Same as output and base
    surface:
      - 10m_u_component_of_wind
      - 10m_v_component_of_wind
      - 2m_temperature
      - mean_sea_level_pressure
      - total_precipitation_6hr
    constants:
      - geopotential_at_surface
      - land_sea_mask
    forcings:
      - toa_incident_solar_radiation
      - sin_time_of_day
      - cos_time_of_day
      - sin_year_progress
      - cos_year_progress

  output:
    atmospheric: ${features.base.atmospheric} # Same as input and base
    surface:
      - 10m_u_component_of_wind
      - 10m_v_component_of_wind
      - 2m_temperature
      - mean_sea_level_pressure
      - total_precipitation_6hr

variable_loss_weights:
  atmospheric:
    u_component_of_wind: 1.0
    geopotential: 1.0
    v_component_of_wind: 1.0
    vertical_velocity: 1.0
    specific_humidity: 1.0
    temperature: 1.0
  surface:
    mean_sea_level_pressure: 0.1
    10m_u_component_of_wind: 0.1
    2m_temperature:          0.1
    total_precipitation_6hr: 0.1
    10m_v_component_of_wind: 0.1
